# Projet simplon

Ce projet implÃ©mente une solution d'analyse de donnÃ©es de ventes pour une PME, utilisant une architecture Ã  deux services Docker pour le traitement et le stockage des donnÃ©es.

## ğŸ— Architecture

Le projet utilise une architecture Ã  deux services avec une configuration rÃ©seau et un flux de donnÃ©es complet :

### Configuration des Services

1. **Service Python (Analyse_des_resultats)**
    - Port: 5000 (rÃ©servÃ© pour futures extensions API)
    - Port: 7860 (interface web Gradio)
    - AccÃ¨s: Web interface accessible depuis l'extÃ©rieur
    - Communication: Bidirectionnelle avec SQLite via volumes Docker

2. **Service SQLite (sqlite_service)**
   - Port: Non exposÃ©
   - AccÃ¨s: Interne uniquement
   - Communication: Via volumes Docker partagÃ©s

### Flux de DonnÃ©es

```mermaid
flowchart TD
    SCHED[Scheduler 12h/00h] -->|DÃ©clenche| FETCH[Data Fetcher]
    CSV1[Ventes CSV] -->|Import| FETCH
    CSV2[Produits CSV] -->|Import| FETCH
    CSV3[Magasins CSV] -->|Import| FETCH
    
    FETCH -->|Import donnÃ©es| PS[Python Service]
    PS -->|Import donnÃ©es| DB[(SQLite DB)]
    PS <-->|RequÃªtes| DB
    
    DB -->|Analyses| AN[Analyses]
    AN -->|RÃ©sultats| LOG[Logs]
    AN -->|Export| CSV[CSV History]
    AN -->|DonnÃ©es| DASH[Dashboard]
    
    subgraph Interface Web
        DASH -->|Ventes| VIZ1[Visualisation Ventes]
        DASH -->|RH| VIZ2[Visualisation RH]
        DASH -->|Produits| VIZ3[Visualisation Produits]
    end

    subgraph Historical Data
        CSV -->|Revenue| H1[revenue.csv]
        CSV -->|Stores| H2[stores.csv]
        CSV -->|Products| H3[products.csv]
        CSV -->|Stock| H4[stock.csv]
    end
```

Le systÃ¨me est composÃ© de :
- **Service Python** : Conteneur exÃ©cutant les scripts d'import et d'analyse
- **Service SQLite** : Base de donnÃ©es stockant et servant les donnÃ©es
- **Flux de DonnÃ©es** : Pipeline automatisÃ© depuis Google Sheets jusqu'aux analyses et exports CSV

## ğŸ“ Structure du projet

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ web_app.py
â”‚   â”œâ”€â”€ csv_exporter.py
â”‚   â”œâ”€â”€ viz-config-ventes.json
â”‚   â”œâ”€â”€ viz-config-magasins.json
â”‚   â”œâ”€â”€ viz-config-produits.json
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ data_fetcher.log
â”œâ”€â”€ data/
â”‚   â””â”€â”€ analysis.db
â”œâ”€â”€ historical_data/
â”‚   â”œâ”€â”€ revenue_history/
â”‚   â”‚   â””â”€â”€ revenue.csv
â”‚   â”œâ”€â”€ store_history/
â”‚   â”‚   â””â”€â”€ stores.csv
â”‚   â”œâ”€â”€ product_history/
â”‚   â”‚   â””â”€â”€ products.csv
â”‚   â””â”€â”€ stock_history/
â”‚       â””â”€â”€ stock.csv
â””â”€â”€ README.md
```

Les fichiers de configuration viz-config-*.json contiennent les paramÃ¨tres de visualisation pour chaque tableau de bord.

## ğŸ—ƒ Structure des donnÃ©es

### Base de donnÃ©es SQLite

Le modÃ¨le de donnÃ©es suit une structure relationnelle avec trois tables principales :

```mermaid
erDiagram
    MAGASINS ||--o{ VENTES : "rÃ©alise"
    PRODUITS ||--o{ VENTES : "concerne"
    
    MAGASINS {
        integer ID_Magasin PK "Primary Key"
        string Ville
        integer Nombre_de_salaries
    }
    
    PRODUITS {
        string ID_Reference_produit PK "Primary Key"
        string Nom
        decimal Prix
        integer Stock
    }
    
    VENTES {
        date Date PK "Part of composite PK"
        string ID_Reference_produit FK "FK & Part of composite PK"
        integer ID_Magasin FK "FK & Part of composite PK"
        integer Quantite
    }
```

### Fichiers CSV

Les donnÃ©es d'analyse sont exportÃ©es dans des fichiers CSV pour un suivi :

1. **revenue.csv**
   - timestamp
   - total_revenue
   - total_employees

2. **stores.csv**
   - timestamp
   - store_id
   - city
   - revenue
   - employee_count

3. **products.csv**
   - timestamp
   - product_name
   - units_sold
   - revenue

4. **stock.csv**
   - timestamp
   - total_units
   - total_value

Chaque fichier est mis Ã  jour avec une nouvelle ligne Ã  chaque analyse (Ã  minuit), permettant un suivi des mÃ©triques clÃ©s.

## ğŸš€ Installation

1. Construire et dÃ©marrer les services :
```bash
docker-compose up --build
```

2. Interagir avec le docker SQlite
```bash
docker exec -it sqlite_service sqlite3 /db/analysis.db
```

## ğŸ“Š FonctionnalitÃ©s

### Import et export des donnÃ©es
- Import automatique et rÃ©gulier des liens Google Sheets (12h et 00h heure de Paris)
- Export des analyses dans des fichiers CSV historiques
- Gestion des doublons
- Validation des donnÃ©es
- Logging dÃ©taillÃ© des opÃ©rations

### Analyses disponibles
1. **Analyses temporelles**
   - Ã‰volution des ventes quotidiennes
   - Tendances par pÃ©riode
   - Jours de forte/faible activitÃ©

2. **Analyses spatiales**
   - Performance par magasin
   - Distribution gÃ©ographique des ventes
   - CorrÃ©lation taille Ã©quipe/performance

3. **Analyses produits**
   - Top des produits vendus
   - Rotation des stocks
   - Chiffre d'affaires par produit

## ğŸ›  Technologies utilisÃ©es

- Python 3.11
- SQLite3
- Docker & Docker Compose
- Pandas pour le traitement des donnÃ©es
- Schedule pour la planification des tÃ¢ches
- Pytz pour la gestion des fuseaux horaires
- Gradio pour l'interface web interactive
- PyGWalker pour les visualisations de donnÃ©es

## ğŸ“Š Dashboard Web

Le projet inclut une interface web interactive accessible via Gradio qui propose trois sections principales :

### 1. Analyse des ventes ğŸ“ˆ
- Visualisations dÃ©taillÃ©es des performances de vente
- Graphiques temporels et tendances
- Filtres interactifs pour l'analyse

### 2. RH & Magasins ğŸ‘¥
- Performances par magasin
- Analyse des effectifs
- Comparaisons et mÃ©triques clÃ©s

### 3. Catalogue produits ğŸ“¦
- Vue d'ensemble du catalogue
- Statistiques de vente par produit
- Analyse des stocks

### AccÃ¨s au dashboard
```bash
# Le dashboard est accessible sur le port 7860
http://localhost:7860
```

CaractÃ©ristiques :
- Interface intuitive avec onglets
- Visualisations interactives
- Filtres dynamiques
- Mise Ã  jour automatique avec les derniÃ¨res donnÃ©es

## ğŸ” Monitoring et maintenance

- Les logs sont disponibles via Docker
- Logs dÃ©taillÃ©s dans `/app/logs/data_fetcher.log`
  * Imports automatiques Ã  12h et 00h (heure de Paris)
  * Statut des opÃ©rations d'import et d'export
  * RÃ©sultats des analyses
- Suivi historique dans `/app/historical_data/`
  * Ã‰volution du chiffre d'affaires
  * Performance des magasins
  * Ventes des produits
  * Ã‰tat des stocks
- Backups automatiques de la base de donnÃ©es
- Surveillance des tÃ¢ches programmÃ©es via les logs Docker
  ```bash
  docker logs -f Analyseur
  ```