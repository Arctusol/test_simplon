# Projet simplon

Ce projet implÃ©mente une solution d'analyse de donnÃ©es de ventes pour une PME, utilisant une architecture Ã  deux services Docker pour le traitement et le stockage des donnÃ©es.

## ğŸ— Architecture

Le projet utilise une architecture Ã  deux services avec une configuration rÃ©seau et un flux de donnÃ©es complet :

### Configuration des Services

1. **Service Python (Analyse_des_resultats)**
    - Port: 5000 (rÃ©servÃ© pour futures extensions API)
    - Port: 7860 (interface web Gradio)
    - AccÃ¨s: Web interface accessible depuis l'extÃ©rieur
    - Communication: Bidirectionnelle avec SQLite via volumes Docker

2. **Service SQLite (sqlite_service)**
   - Port: Non exposÃ©
   - AccÃ¨s: Interne uniquement
   - Communication: Via volumes Docker partagÃ©s

### Flux de DonnÃ©es

```mermaid
flowchart TD
    SCHED[Scheduler 12h/00h] -->|DÃ©clenche| FETCH[Data Fetcher]
    CSV1[Ventes CSV] -->|Import| FETCH
    CSV2[Produits CSV] -->|Import| FETCH
    CSV3[Magasins CSV] -->|Import| FETCH
    
    FETCH -->|Import donnÃ©es| PS[Python Service]
    PS -->|Import donnÃ©es| DB[(SQLite DB)]
    PS <-->|RequÃªtes| DB
    
    DB -->|Analyses| AN[Analyses]
    AN -->|RÃ©sultats| LOG[Logs]
    AN -->|DonnÃ©es| DASH[Dashboard]
    
    subgraph Interface Web
        DASH -->|Ventes| VIZ1[Visualisation Ventes]
        DASH -->|RH| VIZ2[Visualisation RH]
        DASH -->|Produits| VIZ3[Visualisation Produits]
    end
```

Le systÃ¨me est composÃ© de :
- **Service Python** : Conteneur exÃ©cutant les scripts d'import et d'analyse
- **Service SQLite** : Base de donnÃ©es stockant et servant les donnÃ©es
- **Flux de DonnÃ©es** : Pipeline automatisÃ© depuis Google Sheets jusqu'aux analyses

## ğŸ“ Structure du projet

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ web_app.py
â”‚   â”œâ”€â”€ viz-config-ventes.json
â”‚   â”œâ”€â”€ viz-config-magasins.json
â”‚   â”œâ”€â”€ viz-config-produits.json
â”‚   â”œâ”€â”€ data_fetcher.log
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ analysis.db
â””â”€â”€ README.md
```

Les fichiers de configuration viz-config-*.json contiennent les paramÃ¨tres de visualisation pour chaque tableau de bord.

## ğŸ—ƒ Structure des donnÃ©es

Le modÃ¨le de donnÃ©es suit une structure relationnelle avec trois tables principales :

```mermaid
erDiagram
    MAGASINS ||--o{ VENTES : "rÃ©alise"
    PRODUITS ||--o{ VENTES : "concerne"
    
    MAGASINS {
        integer ID_Magasin PK "Primary Key"
        string Ville
        integer Nombre_de_salaries
    }
    
    PRODUITS {
        string ID_Reference_produit PK "Primary Key"
        string Nom
        decimal Prix
        integer Stock
    }
    
    VENTES {
        date Date PK "Part of composite PK"
        string ID_Reference_produit FK "FK & Part of composite PK"
        integer ID_Magasin FK "FK & Part of composite PK"
        integer Quantite
    }
```

CaractÃ©ristiques principales :
- Table **VENTES** avec clÃ© primaire composite sur trois champs (Date, ID_Reference_produit, ID_Magasin)
- Relations one-to-many entre MAGASINS/PRODUITS et VENTES
- Table **PRODUITS** gÃ©rant le catalogue (prix, stocks)
- Table **MAGASINS** stockant les informations gÃ©ographiques et RH
- IntÃ©gritÃ© rÃ©fÃ©rentielle assurÃ©e par les clÃ©s Ã©trangÃ¨res

## ğŸš€ Installation

1. Construire et dÃ©marrer les services :
```bash
docker-compose up --build
```

2. Interagir avec le docker SQlite
```bash
docker exec -it sqlite_service sqlite3 /db/analysis.db
```

## ğŸ“Š FonctionnalitÃ©s

### Import des donnÃ©es
- Import automatique et rÃ©gulier des liens Google Sheets (12h et 00h heure de Paris)
- Gestion des doublons
- Validation des donnÃ©es
- Logging dÃ©taillÃ© des opÃ©rations d'import

### Analyses disponibles
1. **Analyses temporelles**
   - Ã‰volution des ventes quotidiennes
   - Tendances par pÃ©riode
   - Jours de forte/faible activitÃ©

2. **Analyses spatiales**
   - Performance par magasin
   - Distribution gÃ©ographique des ventes
   - CorrÃ©lation taille Ã©quipe/performance

3. **Analyses produits**
   - Top des produits vendus
   - Rotation des stocks
   - Chiffre d'affaires par produit

## ğŸ›  Technologies utilisÃ©es

- Python 3.11
- SQLite3
- Docker & Docker Compose
- Pandas pour le traitement des donnÃ©es
- Schedule pour la planification des tÃ¢ches
- Pytz pour la gestion des fuseaux horaires
- Gradio pour l'interface web interactive
- PyGWalker pour les visualisations de donnÃ©es

## ğŸ“Š Dashboard Web

Le projet inclut une interface web interactive accessible via Gradio qui propose trois sections principales :

### 1. Analyse des ventes ğŸ“ˆ
- Visualisations dÃ©taillÃ©es des performances de vente
- Graphiques temporels et tendances
- Filtres interactifs pour l'analyse

### 2. RH & Magasins ğŸ‘¥
- Performances par magasin
- Analyse des effectifs
- Comparaisons et mÃ©triques clÃ©s

### 3. Catalogue produits ğŸ“¦
- Vue d'ensemble du catalogue
- Statistiques de vente par produit
- Analyse des stocks

### AccÃ¨s au dashboard
```bash
# Le dashboard est accessible sur le port 7860
http://localhost:7860
```

CaractÃ©ristiques :
- Interface intuitive avec onglets
- Visualisations interactives
- Filtres dynamiques
- Mise Ã  jour automatique avec les derniÃ¨res donnÃ©es

##  Utilisation

1. **Importer les donnÃ©es**
```bash
docker-compose exec scripts python import_data.py
```

2. **Lancer les analyses**
```bash
docker-compose exec scripts python analyze_data.py
```

## ğŸ” Monitoring et maintenance

- Les logs sont disponibles via Docker
- Logs dÃ©taillÃ©s des imports programmÃ©s dans `data_fetcher.log`
  * Imports automatiques Ã  12h et 00h (heure de Paris)
  * Statut des opÃ©rations d'import
  * RÃ©sultats des analyses
- Les rÃ©sultats d'analyses sont stockÃ©s dans la table `analyses_resultats`
- Backups automatiques de la base de donnÃ©es
- Surveillance des tÃ¢ches programmÃ©es via les logs Docker
  ```bash
  docker logs -f Analyseur
  ```
